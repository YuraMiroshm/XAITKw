{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a651a8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows usage of the XAITK-Saliency API to gain inference on the behavior of a deep RL agent in an Atari 2600 environment.\n",
    "\n",
    "This example is based upon [this paper](https://arxiv.org/abs/1711.00138) and corresponding [github page](https://github.com/greydanus/visualize_atari).\n",
    "The authors use the Asynchronous Advantage Actor Critic (A3C) algorithm with an LSTM-CNN policy network to train several agents for automated gameplay of different Atari 2600 games.\n",
    "They also implement a method for generating saliency maps using image perturbation.\n",
    "We will show here a recreation of their results using the XAITK-Saliency API, focusing on the Breakout environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ced8f",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411c7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"xaitk-saliency\"\n",
    "!pip install -q \"torchvision\"\n",
    "!pip install -q \"gym[atari]\"\n",
    "!pip install -q \"opencv-python\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548e473",
   "metadata": {},
   "source": [
    "## Download Pretrained Model\n",
    "\n",
    "The author's provide pretrained agents for the different environments they used.\n",
    "We will use the Breakout agent for our purposes.\n",
    "\n",
    "Due to permissions, downloading the zip file with the models will fail.\n",
    "Navigate to the Google drive link manually to download the zip file and move it to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e34376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/u/0/uc?export=download&confirm=gzOH&id=0B-HNE76mR97FaHYtX202WFZSRXc \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/u/0/uc?export=download&confirm=gzOH&id=0B-HNE76mR97FaHYtX202WFZSRXc'\n",
    "output = 'pretrained.zip'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c55751",
   "metadata": {},
   "source": [
    "### Extract Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56595843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_fname = 'pretrained.zip'\n",
    "output_dir = 'pretrained_agents/'\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_fname, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "except FileNotFoundError:\n",
    "    print(\"Can't find zip file. Make sure you have downloaded and moved it to the cwd.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da5fd3",
   "metadata": {},
   "source": [
    "## Create Atari Environment\n",
    "\n",
    "Here we create the Breakout environment for our agent using Gym.\n",
    "\n",
    "Our agent has 4 different actions to choose from:\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;1. Do nothing\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;2. Fire\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;3. Move right\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;4. Move left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf34caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +a54a328)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_name = \"Breakout-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.seed(1)\n",
    "\n",
    "action_space = env.unwrapped.get_action_meanings()\n",
    "print(f\"Action space: {action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5ce04",
   "metadata": {},
   "source": [
    "## Define Policy Network\n",
    "\n",
    "This policy network implementation is taken directly from the author's own implementation.\n",
    "It consists of four convolutional layers, an LSTM layer, and two separate fully-connected layers for the value and policy function predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df77bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "class NNPolicy(torch.nn.Module):  # an actor-critic neural network\n",
    "    def __init__(self, channels, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.lstm = nn.LSTMCell(32 * 5 * 5, 256)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(256, 1), nn.Linear(\n",
    "            256, num_actions\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), (hx, cx)\n",
    "\n",
    "    def try_load(self, save_dir, checkpoint=\"*.tar\"):\n",
    "        paths = glob.glob(save_dir + checkpoint)\n",
    "        step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split(\".\")[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts)\n",
    "            step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step == 0 else print(\n",
    "            \"\\tloaded model: {}\".format(paths[ix])\n",
    "        )\n",
    "        return step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126ed86",
   "metadata": {},
   "source": [
    "## Load Pretrained Model\n",
    "\n",
    "Here we load the downloaded model into an instance of the policy function class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbf6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: pretrained_agents/breakout-v0/strong.40.tar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3d05dd79b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dir = '{}{}/'.format(output_dir, env_name.lower())\n",
    "\n",
    "model = NNPolicy(channels=1, num_actions=env.action_space.n)\n",
    "_ = model.try_load(load_dir, checkpoint='*.tar')\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca132d18",
   "metadata": {},
   "source": [
    "## Define Rollout Function\n",
    "\n",
    "This function carries out the pretrained agent's policy for a defined number of frames in our Breakout environment.\n",
    "At each step, the current game frame is ran through our policy model to get the predicted best action and the agent takes that action.\n",
    "The state of the game is stored after after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e447c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "prepro = (\n",
    "    lambda img: cv2.resize(src=img[35:195].mean(2), dsize=(80, 80))\n",
    "    .astype(np.float32)\n",
    "    .reshape(1, 80, 80)\n",
    "    / 255.0\n",
    ")\n",
    "\n",
    "def rollout(model, env, max_ep_len):\n",
    "    history = {\"ins\": [], \"logits\": [], \"values\": [], \"outs\": [], \"hx\": [], \"cx\": []}\n",
    "\n",
    "    state = torch.Tensor(prepro(env.reset()))  # get first state\n",
    "    episode_length, epr, eploss, done = 0, 0, 0, False  # bookkeeping\n",
    "    hx, cx = torch.zeros(1, 256), torch.zeros(1, 256)\n",
    "\n",
    "    # iterate through each frame in episode\n",
    "    while not done and episode_length <= max_ep_len:\n",
    "        episode_length += 1\n",
    "        \n",
    "        # get game state\n",
    "        model_inp = (state.view(1, 1, 80, 80), (hx, cx))\n",
    "        \n",
    "        # run through model\n",
    "        value, logit, (hx, cx) = model(model_inp)\n",
    "        hx, cx = hx.data, cx.data\n",
    "        \n",
    "        # action probabilities\n",
    "        prob = F.softmax(logit)\n",
    "\n",
    "        # best action\n",
    "        action = prob.max(1)[1].data\n",
    "        \n",
    "        # take best action\n",
    "        obs, reward, done, expert_policy = env.step(action.numpy()[0])\n",
    "        \n",
    "        state = torch.Tensor(prepro(obs))\n",
    "        epr += reward\n",
    "\n",
    "        # save state\n",
    "        history[\"ins\"].append(obs) # game state after taking action\n",
    "        history[\"hx\"].append(hx.squeeze(0).data.numpy()) # LSTM hx output\n",
    "        history[\"cx\"].append(cx.squeeze(0).data.numpy()) # LSTM cx output\n",
    "        history[\"logits\"].append(logit.data.numpy()[0]) # actor output\n",
    "        history[\"values\"].append(value.data.numpy()[0]) # critic output\n",
    "        history[\"outs\"].append(prob.data.numpy()[0]) # action probabilities\n",
    "        print(\"\\tstep # {}, reward {:.0f}\".format(episode_length, epr), end=\"\\r\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c80601",
   "metadata": {},
   "source": [
    "### Play Breakout\n",
    "\n",
    "Our pretrained agent will now play the game for 3,000 frames.\n",
    "We will create a short video clip from a slice of the game state so we can see the agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e46ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling out policy...\n",
      "\tstep # 2716, reward 376\n",
      "Creating video...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Play game\n",
    "print(\"Rolling out policy...\")\n",
    "history = rollout(model, env, max_ep_len=3e3)\n",
    "\n",
    "# Create video from frames\n",
    "print(\"\\nCreating video...\")\n",
    "\n",
    "w = history['ins'][0].shape[1]\n",
    "h = history['ins'][0].shape[0]\n",
    "fps = 30\n",
    "\n",
    "out = cv2.VideoWriter(\"breakout.mp4\", cv2.VideoWriter_fourcc(*'vp09'), fps, (w,h))\n",
    "\n",
    "start_frame = 1000\n",
    "end_frame = 1200\n",
    "for i in range(start_frame, end_frame+1):\n",
    "    frame = cv2.cvtColor(history['ins'][i], cv2.COLOR_RGB2BGR) # Convert to BRG for cv2 standards\n",
    "    out.write(frame)\n",
    "out.release()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188c9b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"50%\" controls>\n",
       "      <source src=\"breakout.mp4\" type=\"video/mp4\">\n",
       "</video></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"50%\" controls>\n",
    "      <source src=\"breakout.mp4\" type=\"video/mp4\">\n",
    "</video></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928053a1",
   "metadata": {},
   "source": [
    "## Defining the Application\n",
    "\n",
    "Our saliency application has four parameters:\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;`start_frame` - the first frame to perform saliency generation for\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;`end_frame` - the last frame to perform saliency generation for\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;`perturber` - the PerturbImage implementation to use\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;`saliency_gen` - the GenerateClassifierConficdenceSaliency implementation to use\n",
    "\n",
    "The application will create saliency maps for both the actor(policy function) and the critic(value function) for each frame from `start_frame` to `end_frame` using the image perturber and saliency generator that you pass it.\n",
    "To show generally what sections of each frame are affecting the agents decisions, the saliency maps are altered in two ways.\n",
    "First, the distinction between negative and positive saliency is removed by taking the absolute value of each map.\n",
    "Second, the saliency maps for each class are averaged to give a single saliency map for the actor and critic of each frame.\n",
    "This gives a single representation for each frame of where both models are looking to make their predictions.\n",
    "Salient parts of each frame will be highlighted in red separately for both the actor and critic and two videos will be created from the respective set of highlighted frames for both models.\n",
    "\n",
    "To speed up this process, the application utilizes multiple processing threads, one for each frame. This will use a good amount of system memory so be cautious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca48352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from xaitk_saliency import PerturbImage, GenerateClassifierConfidenceSaliency\n",
    "from xaitk_saliency.utils.masking import occlude_image_batch\n",
    "import threading\n",
    "\n",
    "actor_sal_maps = []\n",
    "critic_sal_maps = []\n",
    "\n",
    "def app(\n",
    "    start_frame: int,\n",
    "    end_frame: int,\n",
    "    perturber: PerturbImage,\n",
    "    saliency_gen: GenerateClassifierConfidenceSaliency\n",
    "):\n",
    "    global actor_sal_maps\n",
    "    global critic_sal_maps\n",
    "    \n",
    "    # Initialize map arrays to correct size\n",
    "    actor_sal_maps = [None] * (end_frame - start_frame +1)\n",
    "    critic_sal_maps = [None] * (end_frame - start_frame +1)\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    for img_idx in range(start_frame, end_frame+1):\n",
    "        \n",
    "        # Create threads\n",
    "        threads.append(\n",
    "            threading.Thread(\n",
    "                target=gen_sal_maps,\n",
    "                args=[img_idx, (img_idx-start_frame), perturber, saliency_gen]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Start threads\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "        \n",
    "    # Wait for threads to finish\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    # Write out videos\n",
    "    print(\"Writing actor video\")\n",
    "    fps = 1\n",
    "    actor_vid_writer = cv2.VideoWriter(\"breakout_actor_saliency.mp4\", cv2.VideoWriter_fourcc(*'vp09'), fps, (w,h))\n",
    "    critic_vid_writer = cv2.VideoWriter(\"breakout_critic_saliency.mp4\", cv2.VideoWriter_fourcc(*'vp09'), fps, (w,h))\n",
    "    \n",
    "    for img_idx in range(start_frame, end_frame+1):\n",
    "        sal_idx = img_idx-start_frame\n",
    "        \n",
    "        actor_img = history['ins'][img_idx].copy()\n",
    "        critic_img = history['ins'][img_idx].copy()\n",
    "        \n",
    "        # Highlight salient locations in red\n",
    "        actor_img[:,:,0] += (200.0*actor_sal_maps[sal_idx]).astype(\"uint8\")\n",
    "        critic_img[:,:,0] += (200.0*critic_sal_maps[sal_idx]).astype(\"uint8\")\n",
    "        \n",
    "        # Convert to BGR to meet cv2 standard\n",
    "        actor_frame = cv2.cvtColor(actor_img, cv2.COLOR_RGB2BGR)\n",
    "        critic_frame = cv2.cvtColor(critic_img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        actor_vid_writer.write(actor_frame)\n",
    "        critic_vid_writer.write(critic_frame)\n",
    "        \n",
    "    actor_vid_writer.release()\n",
    "    critic_vid_writer.release()\n",
    "    \n",
    "    print(\"Done.\")\n",
    "    \n",
    "def gen_sal_maps(img_idx, sal_idx, perturber, saliency_gen):\n",
    "    global actor_sal_maps\n",
    "    global critic_sal_maps\n",
    "    \n",
    "    # Score reference frame\n",
    "    print(f\"[{img_idx}]Scoring frame\")\n",
    "    \n",
    "    ref_img = history['ins'][img_idx]\n",
    "    \n",
    "    ref_img_proc = prepro(ref_img)\n",
    "\n",
    "    ref_img_tensor = torch.tensor(ref_img_proc.reshape(1, 1, 80, 80))\n",
    "    hx = torch.tensor(history['hx'][img_idx-1]).view(1, -1)\n",
    "    cx = torch.tensor(history['cx'][img_idx-1]).view(1, -1)\n",
    "\n",
    "    model_inp = (ref_img_tensor, (hx, cx))\n",
    "\n",
    "    ref_value, ref_logit, _ = model(model_inp)\n",
    "\n",
    "    ref_value = ref_value.detach().numpy()[0]\n",
    "    ref_logit = ref_logit.detach().numpy()[0]\n",
    "        \n",
    "    # Get image perturbations\n",
    "    print(f\"[{img_idx}]Perturbing image\")\n",
    "\n",
    "    pert_masks = perturber(ref_img)\n",
    "        \n",
    "    pert_imgs = occlude_image_batch(ref_img, pert_masks)\n",
    "\n",
    "    pert_values = []\n",
    "    pert_logits = []\n",
    "    \n",
    "    # Score perturbations\n",
    "    print(f\"[{img_idx}]Scoring perturbations\")\n",
    "\n",
    "    for pert_img in pert_imgs:\n",
    "\n",
    "        pert_img_proc = prepro(pert_img)\n",
    "\n",
    "        pert_img_tensor = torch.tensor(pert_img_proc.reshape(1, 1, 80, 80))\n",
    "\n",
    "        model_inp = (pert_img_tensor, (hx, cx))\n",
    "\n",
    "        pert_value, pert_logit, _ = model(model_inp)\n",
    "\n",
    "        pert_values.append(pert_value.detach().numpy()[0])\n",
    "        pert_logits.append(pert_logit.detach().numpy()[0])\n",
    "        \n",
    "    # Generate actor saliency maps\n",
    "    print(f\"[{img_idx}]Generating actor saliency maps\")\n",
    "    actor_sal = saliency_gen(ref_logit, pert_logits, pert_masks)\n",
    "    actor_sal = np.sum(np.abs(actor_sal), axis=0)\n",
    "    actor_sal = actor_sal / actor_sal.max()\n",
    "    actor_sal_maps[sal_idx] = actor_sal\n",
    "        \n",
    "    # Generate critic saliency maps\n",
    "    print(f\"[{img_idx}]Generating critic saliency maps\")\n",
    "    critic_sal = saliency_gen(ref_value, pert_values, pert_masks)[0]\n",
    "    critic_sal = np.abs(critic_sal)\n",
    "    critic_sal_maps[sal_idx] = critic_sal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180dfd54",
   "metadata": {},
   "source": [
    "## Perturbation and Saliency Implementations\n",
    "\n",
    "For this example we will use the `SlidingWindow` perturbation implementation with a window size of (5,5) and stride of (5,5).\n",
    "\n",
    "The `OcclusionScoring` heatmap generation implementation is appropriate here as both the actor and critic are classification-like models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954163d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xaitk_saliency.impls.perturb_image.sliding_window import SlidingWindow\n",
    "from xaitk_saliency.impls.gen_classifier_conf_sal.occlusion_scoring import OcclusionScoring\n",
    "\n",
    "window_perturber = SlidingWindow(window_size=(5,5), stride=(5,5))\n",
    "sal_gen = OcclusionScoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e93ceb",
   "metadata": {},
   "source": [
    "## Calling the Application\n",
    "\n",
    "Five arbitrary frames from the middle of the set are chosen for saliency generation using our application.\n",
    "The resulting videos will be displayed side-by-side, actor on the right, critic on the left, after the application finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9bdbc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1039]Scoring frame\n",
      "[1040]Scoring frame\n",
      "[1041]Scoring frame\n",
      "[1042]Scoring frame[1043]Scoring frame\n",
      "\n",
      "[1039]Perturbing image\n",
      "[1040]Perturbing image\n",
      "[1043]Perturbing image\n",
      "[1041]Perturbing image\n",
      "[1042]Perturbing image\n",
      "[1039]Scoring perturbations\n",
      "[1040]Scoring perturbations\n",
      "[1043]Scoring perturbations\n",
      "[1042]Scoring perturbations\n",
      "[1041]Scoring perturbations\n",
      "[1040]Generating actor saliency maps\n",
      "[1039]Generating actor saliency maps\n",
      "[1043]Generating actor saliency maps\n",
      "[1042]Generating actor saliency maps\n",
      "[1041]Generating actor saliency maps\n",
      "[1040]Generating critic saliency maps\n",
      "[1039]Generating critic saliency maps\n",
      "[1043]Generating critic saliency maps\n",
      "[1042]Generating critic saliency maps\n",
      "[1041]Generating critic saliency maps\n",
      "Writing actor video\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "app(\n",
    "    start_frame=1039,\n",
    "    end_frame=1043,\n",
    "    perturber=window_perturber,\n",
    "    saliency_gen=sal_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e1fb990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"left\">\n",
       "<video width=\"45%\" controls>\n",
       "    <source src=\"breakout_actor_saliency.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "<video width=\"45%\" controls>\n",
       "    <source src=\"breakout_critic_saliency.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"left\">\n",
    "<video width=\"45%\" controls>\n",
    "    <source src=\"breakout_actor_saliency.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=\"45%\" controls>\n",
    "    <source src=\"breakout_critic_saliency.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a20662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_venv",
   "language": "python",
   "name": "local_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
